{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["o5bKKVUG6SA5"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Computación clásica"],"metadata":{"id":"bnqEAj0sYiHt"}},{"cell_type":"markdown","source":["## Un poco de historia"],"metadata":{"id":"vdmv9PrPYnrX"}},{"cell_type":"markdown","source":["Las primeras herramientas para representar información y para realizar operaciones aritméticas datan de hace más de 4000 años.\n","\n","![Quipu](https://drive.google.com/uc?export=view&id=1ISpvXKtjzzrUMWHHb5zJhnQt40YxMrum)\n","\n","[Quipu](https://es.wikipedia.org/wiki/Quipu) (año -2500): Sistema Inca para aritmética simple y almacenamiento de información.\n","\n","![Ábaco](https://drive.google.com/uc?export=view&id=1m0tiQj4DdjbCrJh52tIXUfKfxNjmFqLT)\n","\n","Ábaco (año -2700 a -2300) Babilonia: sumas restas, multiplicaciones y divisiones.\n","\n","![](https://drive.google.com/uc?export=view&id=1Vq5yCEeE3UK-FUADdubM1f3ruZdXf53K)\n","\n","Blaise Pascal (1642) Suma, resta, multiplicación y división automáticas.\n","\n","\n","![](https://drive.google.com/uc?export=view&id=1Yr9a4iax05ikuBmV6BPyGJ-7rRj1gNo1)\n","\n","Leonardo Torres Quevedo (1912): el ajedrecista, Calculadora electromecánica. Resolución de ecuaciones cuadráticas, etc.\n","\n","\n"],"metadata":{"id":"ZD7p12EfYrYE"}},{"cell_type":"markdown","source":["##Máquinas de Turing\n","\n","\n","\n"],"metadata":{"id":"u9BCN0McxKLT"}},{"cell_type":"markdown","source":["<p align =\"justify\">Una de las herramientas fundamentales de la teoría de la computación es la máquina de Turing (TM), un modelo abstracto de computación propuesto por <a href=\"https://es.wikipedia.org/wiki/Alan_Turing\"> Alan Turing</a> en 1936. La máquina de Turing es una forma de representar formalmente el concepto de algoritmo, es decir, una serie finita de pasos que se siguen para resolver un problema.<p>\n","\n","<p align =\"justify\">Una <a href=https://www.youtube.com/watch?v=FTSAiF9AHN4> máquina de Turing</a> se compone de tres partes: una cinta infinita que sirve como memoria, un cabezal que puede leer y escribir símbolos en la cinta, y una tabla (programa) que indica cómo debe comportarse el cabezal según el símbolo leído y el estado interno de la máquina. La cinta está dividida en celdas, cada una de las cuales puede contener un símbolo de un alfabeto finito. El cabezal puede moverse a la izquierda o a la derecha sobre la cinta, leyendo o escribiendo un símbolo en cada movimiento. La tabla es una lista finita de reglas que determinan qué hacer en cada situación: qué símbolo escribir, en qué dirección moverse y a qué estado cambiar. <p>\n","\n","<p align =\"justify\">El funcionamiento de una máquina de Turing es muy simple: se parte de una configuración inicial: la cinta contiene una entrada y el cabezal está situado en una posición determinada. A partir de ahí, el cabezal lee el símbolo actual y consulta la tabla para saber qué hacer. Luego ejecuta la acción correspondiente y pasa a la siguiente configuración. Este proceso se repite hasta que la máquina llega a un estado especial llamado estado final o estado de parada, que indica que la computación ha terminado. El resultado del cómputo se puede leer en la cinta.  <p>"],"metadata":{"id":"kfoWxgLZcu9X"}},{"cell_type":"markdown","source":["### Computación Universal\n","<p align=\"justify\">Una máquina de Turing universal es una máquina de Turing capaz de simular cualquier otra máquina de Turing. Esto significa que puede ejecutar cualquier algoritmo que se pueda expresar mediante una máquina de Turing. Para ello, hay que codificar la tabla (de la máquina que se quiere simular) y los datos de entrada como símbolos en la cinta y usar una <b>tabla especial</b> que permita interpretarlos y ejecutarlos. Lo que logró Turing con su máquina universal fue  demostrar que existe un modelo único y universal de computación, que puede realizar todas las tareas computables que se puedan imaginar. No es necesario modificar la mecánica de la máquina (como se pensaba en la época) para cambiar el problema que se busca resolver. Además, la máquina de Turing es particularmente simple. <p>\n","\n","<p align=\"justify\">Turing demostró que cualquier problema que pueda ser resuelto por un programa de computadora puede ser resuelto por una máquina de Turing y, por lo tanto, puede ser resuelto por cualquier computadora moderna.\n","<p>\n","<p align=\"justify\">Turing además mostró que existen problemas que no son resolubles por una computadora. Por ejemplo, no existe ningún algoritmo que permita analizar cualquier programa con su entrada de datos y pueda decidir si el programa se detiene (termina su ejecución) luego de un número finito de pasos o sigue para siempre. Este problema se conoce como el problema de la parada o detención (Halting problem) de Turing. La importancia de este resultado radica en que establece los límites teóricos de lo que se puede computar y lo que no.<p>\n","\n","<p align=\"justify\">¿Qué problemas se podrán resolver con una TM?\n","Turing junto con Alonzo Church (director de tesis de AT y que hizo una formulación de la computación equivalente a la de Turing) formularon la tesis de Church-Turing: Todo lo que puede ser considerado como “naturalmente” computable (o efectivamente calculable), puede ser calculado con una máquina de Turing ¡Aunque la máquina puede parecer rudimentaria, es capaz de hacer cualquier cálculo que haga una computadora! Hasta ahora no hay nada que contradiga esa tesis, ni siquiera la computación cuántica. Sin embargo, si se incluye la noción de eficiencia (que definiremos más abajo), la computación cuántica sí parece cambiar las cosas (aunque no ha sido demostrado que así sea).\n","\n"," Es muy costoso computacionalmente, pero no imposible, simular sistemas cuánticos con una computadora clásica. Esta fue originalmente la motivación de Richard Feynman para proponer la construcción de computadoras cuánticas o aún más cerca de la motivación original, simuladores cuánticos. Construyendo una computadora cuántica uno podría simular sistemas cuánticos de manera mucho menos costosa computacionalmente. No se conoce ninguna manera de simular de manera eficiente una computadora cuántica utilizando una computadora clásica: El tamaño de la computadora clásica necesaria crece de manera polinómica con el tamaño (e.g. número de qubits) de la computadora cuántica pero el tiempo necesario lo hace de manera exponencial con dicho tamaño. Más interesante aún, al igual que con la computación clásica, es posible definir una computadora cuántica universal (regida por las leyes de la mecánica cuántica) que puede realizar cualquier operación permitida por la Mecánica cuántica. Hay indicios de que la computación cuántica es más poderosa que la clásica ya que existen algoritmos para los cuales no se conoce uno clásico que sea *eficiente*. Ej.: algoritmo de factorización de Peter Shor.\n","La computación cuántica pone en duda a la llamada tesis fuerte de Church-Turing: “Una máquina de Turing puede simular eficientemente cualquier modelo razonable de computación”.<p>"],"metadata":{"id":"msc0aIaL33JP"}},{"cell_type":"markdown","source":["##Computación clásica digital\n"],"metadata":{"id":"FG9lVCcUKHGl"}},{"cell_type":"markdown","source":["En una computadora clásica digital codificamos la información en una secuencia de ceros y unos (bits) de un tamaño dado ($n$) y esperamos leer el resultado como otra secuencia de $m$ bits.\n","$$\n","f:\\{0,1\\}^n \\to \\{0,1\\}^m\n","$$\n","Para simplificar el análisis es posible concentrarse en los llamados problemas de decisión,\n","$$f:\\{0,1\\}^n \\to \\{0,1\\},$$ tales que para una entrada arbitraria de $n$ bits, dan una respuesta booleana $0$ o $1$ (no o sí).\n","Cualquier problema se puede representar como uno de decisión (en particular podemos obtener una salida de $m$ bits como el resultado de $m$ problemas de decisión). Por ejemplo, la factorización se puede plantear haciendo preguntas del tipo: cuál es el bit i-ésimo del menor factor primo de un número, etc. Lo que puede verse afectado negativamente al plantear los problemas de esta forma es la eficiencia.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"frqUqRZ84oxH"}},{"cell_type":"markdown","source":["### Compuertas lógicas"],"metadata":{"id":"TOu_r5VgKYL2"}},{"cell_type":"markdown","source":["\n","La evaluación de cualquier problema de decisión $f(x)$ se puede reducir a una secuencia de operaciones lógicas. Dada una entrada $x = x_1 \\ldots x_n$ de $n$ bits, podemos clasificarla según $f(x)$ sea $0$ o $1$ y armar dos conjuntos. Dado el conjunto de valores de $x(a)$ tales que $f(x(a))=1$ (con $a = 1, \\ldots , k$) definimos las funciones:\n","$$f^{(a)}(x)= \\delta_{x,x^{(a)}}$$\n","donde $\\delta_{x,x^{(a)}}$ es la delta de Kronecker que vale **uno** is $x= x^{(a)}$ y **cero** en otro caso.\n","\n","Usando estas funciones podemos escribir la función original usando la puerta $\\text{OR}$ (disyunción lógica $\\lor$)\n","$$\n","\\begin{array}{|c|c|c|}\n","\\hline\n","A & B & A \\lor B \\\\\n","\\hline\n","0 & 0 & 0 \\\\\n","0 & 1 & 1 \\\\\n","1 & 0 & 1 \\\\\n","1 & 1 & 1 \\\\\n","\\hline\n","\\end{array}\n","$$\n","\n","$$f(x)=f^{(1)}(x) \\lor f^{(2)}(x) \\lor f^{(3)}(x) \\lor \\ldots\\lor f^{(k)}(x)$$\n","\n","\n","\n","\n","Además, podemos escribir a cualquier  $f^{(a)}(x)$ usando la puerta $\\text{AND}$ (conjunción lógica $\\land$)\n","\n","\\begin{array}{|c|c|c|}\n","\\hline\n","A & B & A \\land B \\\\\n","\\hline\n","0 & 0 & 0 \\\\\n","0 & 1 & 0 \\\\\n","1 & 0 & 0 \\\\\n","1 & 1 & 1 \\\\\n","\\hline\n","\\end{array}\n","\n","y la negación $\\text{NOT}$ ($\\lnot$),\n","$$\n","\\begin{array}{|c|c|}\n","\\hline\n","A & \\lnot A \\\\\n","\\hline\n","0 & 1 \\\\\n","1 & 0 \\\\\n","\\hline\n","\\end{array}\n","$$\n","\n","Por ejemplo:\n"," $$f^{(a)}(x)=x_1 \\land x_2 \\land x_3 \\land x_4 \\land \\ldots \\land x_n $$\n","es igual a $1$ solamente si $x=1 1 1 1 1 1 \\ldots 1$.\n","\n","$$f^{(a)}(x)=\\lnot x_1 \\land \\lnot x_2 \\land \\lnot x_3 \\land \\lnot x_4 \\land  \\ldots \\land \\lnot x_n $$\n","es $1$ solamente si $x=000000\\ldots 0$ (todos los bits de $x$ son cero).\n","\n","y para un valor arbitrario de  $x^{(a)}= 0110\\ldots 1$ tenemos:\n","$$f^{(a)}(x)=\\lnot x_1 \\land x_2 \\land x_3 \\land \\lnot x_4 \\land \\ldots \\land x_n $$\n","\n","aplicando la negación o no dependiendo del valor del bit i-ésimo de  $x(a)$.\n","\n","Esto muestra que podemos escribir cualquier función de decisión utilizando $\\text{OR}$, $\\text{AND}$ y $\\text{NOT}$, que forman por lo tanto un **conjunto universal** de compuertas lógicas.\n","\n","Esto es equivalente a codificar una tabla con todas las posibles entradas y salidas, lo que no es un algoritmo muy útil en general, pero la idea de base es que podemos dar la solución a cualquier problema utilizando solo esas compuertas (y la posibilidad de copiar bits), lo que simplifica enormemente la electrónica (se puede demostrar que el número mínimo de compuertas lógicas es $1$). Por ejemplo, la compuerta NAND:\n","\n","\n","\\begin{array}{|c|c|c|}\n","\\hline\n","A & B & \\lnot (A \\land B) \\\\\n","\\hline\n","0 & 0 & 1 \\\\\n","0 & 1 & 1 \\\\\n","1 & 0 & 1 \\\\\n","1 & 1 & 0 \\\\\n","\\hline\n","\\end{array}\n","\n","Las operaciones $\\text{NOT}$, $\\text{AND}$ y $\\text{OR}$ se pueden construir usando $\\text{NAND}$\n","- $\\lnot A = A \\, \\text{NAND} \\, A $\n","- $A \\land B \\lnot (A \\, \\text{NAND} \\, B) $\n","- $ A \\lor B = (\\lnot A) \\, \\text{NAND} \\, (\\lnot B) $\n"],"metadata":{"id":"DBk7_zJuKNKH"}},{"cell_type":"markdown","source":["### Complejidad"],"metadata":{"id":"y4atSsidKDn6"}},{"cell_type":"markdown","source":["\n","<p align=\"justify\">Del conjunto de problemas que sí se pueden resolver, queremos definir clases de complejidad que permitan clasificar a los problemas como \"fáciles\" o “difíciles”. La dificultad se puede cuantificar con los recursos necesarios físicos (medidos en número de compuertas), tiempo y la energía necesaria. Esta clasificación permite comparar y analizar la dificultad de los problemas y las posibilidades de diseñar algoritmos eficientes para resolverlos.<p>\n","\n","<p align=\"justify\">Para la clasificación hay que tener en cuenta que el tiempo y espacio están relacionados (se puede hacer un procesamiento en paralelo para reducir el número de pasos de tiempo) por lo que es conveniente medir los recursos necesarios (o el tiempo) con el número de operaciones elementales.\n","Para clasificar los problemas (o los algoritmos) se estudia cómo crece el número de operaciones con el número de bits de entrada (por ejemplo, el número de bits de un entero que se quiere factorizar). Se dice que un algoritmo para resolver un problema es “eficiente” si el tiempo T crece más lento que un polinomio en el número de bits de la entrada ($n$): $T < cn^k$ con $c$ y $k$ constantes. La notación asintótica para $n$ grande es $\\mathcal{O}(n^k)$. Para la clasificación se considera lo siguiente:<p>\n","\n","* El peor escenario posible, la entrada de $n$ bits más complicada.\n","* El algoritmo tiene que modificarse de manera $uniforme$ al aumentar $n$. Tiene que ser fácil construir el algoritmo para $n + 1$ a partir del de $n$.\n","* Distintos conjuntos de compuertas elementales dan un número distinto para las operaciones necesarias, pero esto no cambia la jerarquía polinomial.\n","\n","Se define la clase de complejidad $P$ como aquella que contiene a todos los problemas que se pueden resolver en tiempo (número de operaciones) polinomial (<i>problemas fáciles</i>). Ejemplos de problemas en $P$:\n","* Sumar dos enteros $\\mathcal{O}(n)$\n","* Multiplicar dos enteros $\\mathcal{O}(n^2)$.\n","* Ordenar un conjunto de $N$ números $\\mathcal{O}(N \\log(N))$\n","* Determinar si un número $N$ es primo o no $\\mathcal{O}(\\log(N)^6)$.\n","\n","La mayoría de las funciones $f:\\{0,1\\}^n\\to \\{0,1\\}$   no están en $P$ porque no hay mejor forma de evaluarlas que buscar en una tabla los valores. Para codificar dicha tabla sería necesario un espacio exponencialmente grande $\\mathcal{O}(2^n)$ y tardaríamos un tiempo exponencialmente grande en generarla. Los problemas que están en $P$ tienen suficiente estructura, tal que la función $f$ puede ser calculada de manera eficiente.\n","\n","Otra clase de problemas llamada $NP$ incluye aquellos para los que es fácil verificar si una solución es correcta pero no necesariamente se conoce algún algoritmo eficiente para resolverlos. Un ejemplo de problema que está en $NP$ es la factorización, si se conocen los factores solo hay que multiplicarlos (que es polinomial) para verificar si la respuesta es correcta o no. Claramente $P$ está contenido en $NP$ , si es fácil encontrar una solución también es fácil comprobar que lo es: $P ⊆ N P$ . La conjetura central de la teoría de complejidad es que $P$ es distinto de $NP$.\n","\n","Dentro de la clase $NP$, hay algunos problemas que son especialmente difíciles, en el sentido de que si se pudiera resolver uno de ellos en tiempo polinomial, se podría resolver cualquier otro problema $NP$ en tiempo polinomial. Estos problemas se llaman $NP$-completos, y se cree que no pertenecen a la clase $P$, aunque no se ha podido demostrar. Un ejemplo de problema $NP$-completo es el sudoku.\n","\n","<p align=\"justify\">Una pregunta abierta y central en la teoría de la complejidad es si para todo problema que permite verificar una solución en tiempo polinomial (está en $NP$) es posible encontrar la solución en tiempo polinomial (está en $P$). La relación entre estas dos clases es uno de los temas más importantes y abiertos de la teoría de la computación, ya que implica cuestiones fundamentales sobre la naturaleza y los límites del cómputo. El problema de si $P=^?NP$ es uno de los problemas del milenio (hay un premio de la <a href=\"http://www.claymath.org\">fundación Clay</a>  de 1M U$\\$$D). Si se demostrara que $P=NP$, significaría que hay algoritmos eficientes para resolver problemas que hoy en día se consideran intratables, como el problema del viajante o el problema de factorización de números grandes. Estos problemas tienen aplicaciones en campos tan diversos como la criptografía, la inteligencia artificial, la biología o la física. Por ejemplo, si $P=NP$, se podría romper cualquier sistema de cifrado basado en la dificultad de factorizar números grandes, como el RSA. También se podría encontrar la mejor ruta para visitar un conjunto de ciudades, obtener la configuración óptima de un sistema físico, o diseñar una molécula con propiedades deseadas. En otras palabras, si $P=NP$, se podrían resolver todos los problemas de la física con un algoritmo eficiente. Esto tendría consecuencias profundas para la ciencia, la tecnología y la sociedad. Sin embargo, la mayoría de los expertos cree que $P≠NP$, es decir, que hay problemas que se pueden verificar rápidamente pero no se pueden resolver rápidamente. Esto implicaría que hay límites fundamentales para lo que se puede computar y para lo que se puede conocer. Esto significaría que hay una diferencia esencial entre el proceso creativo y el proceso analítico tanto en el arte como en la ciencia, entre componer una sinfonía y leerla, o entre crear una teoría física y entenderla.<p>\n"],"metadata":{"id":"iITLIPN78gYU"}},{"cell_type":"markdown","source":["## Energía y computación"],"metadata":{"id":"lS4vQCF_J_gu"}},{"cell_type":"markdown","source":["\n","\n","Hasta ahora no hablamos del costo en energía. Las compuertas $\\text{OR}$ y $\\text{AND}$ son irreversibles: a partir del resultado, no puedo inferir los bits de entrada y pierdo información al aplicar la compuerta (la compuerta $\\text{NOT}$ sí es reversible).\n","\n","[Landauer (1961)](http://worrydream.com/refs/Landauer%20-%20Irreversibility%20and%20Heat%20Generation%20in%20the%20Computing%20Process.pdf) mostró que la irreversibilidad tiene un costo asociado en energía. Supongamos que tenemos un bit clásico en $0$ o en $1$ y queremos ponerlo en cero (borrarlo), esto es lo mismo que eliminar la información como ocurre en una compuerta irreversible.\n","\n","Landauer usó un modelo mecánico para codificar la información, pero resulta en general más claro usar un modelo termodinámico. Supongamos que tenemos una caja con un tabique en el medio que la separa en dos partes. Asociamos el estado $0$ a la situación en la que todas partículas de un gas están a la izquierda del tabique y $1$ cuando están todas a la derecha. Si observo el estado del sistema antes de comenzar, puedo borrar (poner en cero el bit) la información de manera reversible: si el gas está a la izquierda, no hago nada; si el gas está a la derecha muevo un pistón de derecha a izquierda y al mismo tiempo muevo el tabique que estaba inicialmente en el medio.  \n","El problema es que al hacer esto **no** estoy borrando realmente la información porque estoy haciendo una copia (al menos en mi cabeza) al observar el sistema.\n","\n","Es posible mostrar que si uno **no** conoce el estado del sistema, lo más eficiente para borrar la información es sacar el tabique central y luego comprimir el gas. Esto produce primero un aumento en la entropía que luego es reducida a costa de realizar trabajo sobre el sistema. Para un gas ideal tenemos\n","\\begin{equation}\n","\tP V= N k_B T\n","\\end{equation}\n","donde $P$ es la presión, $V$ el volumen del gas, $N$ el número de partículas y $T$ la temperatura.\n","\n","Suponemos un proceso isotérmico en el que comprimimos el gas hacia la izquierda para luego reponer el tabique.\n","\n","![Pistón](https://phys.libretexts.org/@api/deki/files/7901/CNX_UPhysics_20_02_WorkByExp.jpg?revision=1)\n","\n","El trabajo realizado es (integrando fuerza $\\times$ distancia):\n","\\begin{equation}\n","    W=-\\int_0^{L/2} P A dx\n","\\end{equation}\n","\n","donde $A$ es el área de la sección transversal de la caja. Usando la ley de un gas ideal y que $V=A(L-x)$ tenemos\n","\\begin{equation}\n","    W=-\\int_0^{L/2} N k_B T/(L-x) dx= N k_B T\\ln(1/2)= -N k_B T\\ln(2)\n","\\end{equation}\n","es el trabajo perdido en el proceso irreversible que ocurre al sacar el tabique.\n","$\\Delta S=N k_B \\ln 2$ es la reducción de entropía del sistema. La mejor situación para reducir el costo en energía es cuando $N=1$ (hay una sola partícula en la caja).\n","\n","Esto resuelve la paradoja del demonio de Maxwell que parece contradecir la segunda ley de la termodinámica: (es imposible construir una máquina que trabaje en un ciclo y que no tenga otro efecto que sacar calor de un único cuerpo para producir trabajo). El demonio tiene que observar las partículas para decidir abrir la puerta y dejar pasar alguna, entonces o recuerda para siempre la que vio o disipa energía de manera irreversible que compensa el posible trabajo ganado llevando las partículas de una lado de la caja al otro.\n","\n","![Demonio](https://drive.google.com/uc?export=view&id=1A3vAu_FI2b8hn3jJjiBVcYlPMDRAqW_L)\n","\n","Este costo energético pone un límite a la computación irreversible. En la computadoras actuales, la disipación es dos órdenes de magnitud mayor a la mínima teórica, por lo que este límite esta todavía lejos de ser un problema.\n","\n","Una opción para reducir este costo es enfriar el sistema a temperaturas cada vez más bajas, pero enfriar cuesta energía...\n","\n","De todas maneras, se puede demostrar que es posible hacer computación reversible usando [compuertas reversibles](https://link.springer.com/chapter/10.1007/3-540-10003-2_104).\n","\n","Para resolver un problema muy costoso computacionalmente en un \"corto tiempo\", podríamos poner la computadora a andar y irnos a hacer un viaje a velocidad cercana a la de la luz, al regreso habrán transcurrido miles de años y podemos leer el resultado. Un problema de esto es el costo en energía que cuesta el paseo relativista, aunque pudiéramos hacer todos los cómputos de manera reversible."],"metadata":{"id":"xzhBktGDffmR"}}]}